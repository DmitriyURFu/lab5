# –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ –∫ —É—Ä–æ–∫—É 1: –û—Å–Ω–æ–≤—ã PyTorch

## –¶–µ–ª—å –∑–∞–¥–∞–Ω–∏—è
–ó–∞–∫—Ä–µ–ø–∏—Ç—å –Ω–∞–≤—ã–∫–∏ —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–Ω–∑–æ—Ä–∞–º–∏ PyTorch, –∏–∑—É—á–∏—Ç—å –æ—Å–Ω–æ–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –∏ –Ω–∞—É—á–∏—Ç—å—Å—è —Ä–µ—à–∞—Ç—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏.

## –ó–∞–¥–∞–Ω–∏–µ 1: –°–æ–∑–¥–∞–Ω–∏–µ –∏ –º–∞–Ω–∏–ø—É–ª—è—Ü–∏–∏ —Å —Ç–µ–Ω–∑–æ—Ä–∞–º–∏ (25 –±–∞–ª–ª–æ–≤)

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `homework_tensors.py` –∏ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–¥–∞—á–∏:

### 1.1 –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–æ–≤ (7 –±–∞–ª–ª–æ–≤)
```python
# –°–æ–∑–¥–∞–π—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ —Ç–µ–Ω–∑–æ—Ä—ã:
# - –¢–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–æ–º 3x4, –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–π —Å–ª—É—á–∞–π–Ω—ã–º–∏ —á–∏—Å–ª–∞–º–∏ –æ—Ç 0 –¥–æ 1
rand_tensor = torch.rand(3,4)

# - –¢–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–æ–º 2x3x4, –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–π –Ω—É–ª—è–º–∏
zero_tensor = torch.zero(2,3,4)

# - –¢–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–æ–º 5x5, –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–π –µ–¥–∏–Ω–∏—Ü–∞–º–∏
ones_tensor = torch.ones(5,5)

# - –¢–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–æ–º 4x4 —Å —á–∏—Å–ª–∞–º–∏ –æ—Ç 0 –¥–æ 15 (–∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ reshape)
0_to_15_tensor = torch.arange(16).reshape(4,4)
```

### 1.2 –û–ø–µ—Ä–∞—Ü–∏–∏ —Å —Ç–µ–Ω–∑–æ—Ä–∞–º–∏ (6 –±–∞–ª–ª–æ–≤)
```python
# –î–∞–Ω–æ: —Ç–µ–Ω–∑–æ—Ä A —Ä–∞–∑–º–µ—Ä–æ–º 3x4 –∏ —Ç–µ–Ω–∑–æ—Ä B —Ä–∞–∑–º–µ—Ä–æ–º 4x3
A = torch.arange(12).reshape(3,4)
B = torch.arange(12).reshape(4,3)

# –í—ã–ø–æ–ª–Ω–∏—Ç–µ:
# - –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–∞ A
transp_tensor = A.T

# - –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ A –∏ B
multiply_A_and_B = A@B

# - –ü–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ A –∏ —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ B
elemnts_multiply = A*B.T

# - –í—ã—á–∏—Å–ª–∏—Ç–µ —Å—É–º–º—É –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ç–µ–Ω–∑–æ—Ä–∞ A
sum_elements_A = A.sum()
```

### 1.3 –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∏ —Å—Ä–µ–∑—ã (6 –±–∞–ª–ª–æ–≤)
```python
# –°–æ–∑–¥–∞–π—Ç–µ —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–æ–º 5x5x5
cube_tensor = torch.arange(125).reshape(5,5,5)

# –ò–∑–≤–ª–µ–∫–∏—Ç–µ:
# - –ü–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É
first_str_tensor = cube_tensor[0,:,:]

# - –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å—Ç–æ–ª–±–µ—Ü
last_clumn_tensor = cube_tensor[:, -1 ,:]

# - –ü–æ–¥–º–∞—Ç—Ä–∏—Ü—É —Ä–∞–∑–º–µ—Ä–æ–º 2x2 –∏–∑ —Ü–µ–Ω—Ç—Ä–∞ —Ç–µ–Ω–∑–æ—Ä–∞
matrix_2x2 = cube_tensor [2:4, 2:4, ;]

# - –í—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å —á–µ—Ç–Ω—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏
even_index_elements = cube_tensor[::2,::2,::2]
```

### 1.4 –†–∞–±–æ—Ç–∞ —Å —Ñ–æ—Ä–º–∞–º–∏ (6 –±–∞–ª–ª–æ–≤)
```python
# –°–æ–∑–¥–∞–π—Ç–µ —Ç–µ–Ω–∑–æ—Ä —Ä–∞–∑–º–µ—Ä–æ–º 24 —ç–ª–µ–º–µ–Ω—Ç–∞
tensor_24 = torch.arange(24)

# –ü—Ä–µ–æ–±—Ä–∞–∑—É–π—Ç–µ –µ–≥–æ –≤ —Ñ–æ—Ä–º—ã:
# - 2x12
tensor_2x12 = tensor_24.reshape(2, 12)

# - 3x8
tensor_3x8 = tensor_24.reshape(3, 8)

# - 4x6
tensor_4x6 = tensor_24.reshape(4, 6)

# - 2x3x4
tensor_2x3x4 = tensor_24.reshape(2, 3, 4)

# - 2x2x2x3
tensor_2x2x2x3 = tensor_24.reshape(2, 2, 2, 3)
```

## –ó–∞–¥–∞–Ω–∏–µ 2: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏–µ (25 –±–∞–ª–ª–æ–≤)

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `homework_autograd.py`:

### 2.1 –ü—Ä–æ—Å—Ç—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º–∏ (8 –±–∞–ª–ª–æ–≤)
```python
# –°–æ–∑–¥–∞–π—Ç–µ —Ç–µ–Ω–∑–æ—Ä—ã x, y, z —Å requires_grad=True
tensor_x = torch.tensor(1, requires_grad=True)
tensor_y = torch.tensor(2, requires_grad=True)
tensor_z = torch.tensor(3, requires_grad=True)

# –í—ã—á–∏—Å–ª–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é: f(x,y,z) = x^2 + y^2 + z^2 + 2*x*y*z
f = x**2 + y**2 + z**2 + 2*x*y*z

# –ù–∞–π–¥–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ –≤—Å–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º
grad_x = tensor_x.grad.item()
grad_y = tensor_y.grad.item()
grad_z = tensor_z.grad.item()

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏
analytical_grad_x = 2*tensor_x.item() + 2*tensor_y.item()*tensor_z.item()
analytical_grad_y = 2*tensor_y.item() + 2*tensor_x.item()*tensor_z.item()
analytical_grad_z = 2*tensor_z.item() + 2*tensor_x.item()*tensor_y.item()

if (grad_x == analytical_grad_x):
    print('X true')
if (grad_y == analytical_grad_y):
    print('Y true')
if (grad_z == analytical_grad_z):
    print('Z true')
```

### 2.2 –ì—Ä–∞–¥–∏–µ–Ω—Ç —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å (9 –±–∞–ª–ª–æ–≤)
```python
# –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é MSE (Mean Squared Error):
# MSE = (1/n) * Œ£(y_pred - y_true)^2
# –≥–¥–µ y_pred = w * x + b (–ª–∏–Ω–µ–π–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è)
# –ù–∞–π–¥–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ w –∏ b
def mse_loss_and_grads(x, y_true, w, b):
    y_pred = w * x + b

    loss = torch.mean((y_pred - y_true) ** 2)

    loss.backward()
    
    grad_w = w.grad.item()
    grad_b = b.grad.item()
    
    return loss.item(), grad_w, grad_b
    
x = torch.tensor([1.0, 2.0, 3.0, 4.0])
y_true = torch.tensor([2.1, 3.9, 6.0, 8.1])

w = torch.tensor(1.5, requires_grad=True)
b = torch.tensor(0.5, requires_grad=True)

loss, grad_w, grad_b = mse_loss_and_grads(x, y_true, w, b)
```

### 2.3 –¶–µ–ø–Ω–æ–µ –ø—Ä–∞–≤–∏–ª–æ (8 –±–∞–ª–ª–æ–≤)
```python
# –†–µ–∞–ª–∏–∑—É–π—Ç–µ —Å–æ—Å—Ç–∞–≤–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é: f(x) = sin(x^2 + 1)
def composite_function(x):
    return torch.sin(x**2 + 1)
x = torch.tensor(2.0, requires_grad=True)
f = composite_function(x)

# –ù–∞–π–¥–∏—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç df/dx
grad_x = x.grad.item()

# –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –ø–æ–º–æ—â—å—é torch.autograd.grad
f.backward(retain_graph=True)
grad_autograd = torch.autograd.grad(f, x)[0].item()
if (grad_x == grad_autograd):
    print('True')
```

## –ó–∞–¥–∞–Ω–∏–µ 3: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ CPU vs CUDA (20 –±–∞–ª–ª–æ–≤)

–°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `homework_performance.py`:

### 3.1 –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö (5 –±–∞–ª–ª–æ–≤)
```python
# –°–æ–∑–¥–∞–π—Ç–µ –±–æ–ª—å—à–∏–µ –º–∞—Ç—Ä–∏—Ü—ã —Ä–∞–∑–º–µ—Ä–æ–≤:
# - 64 x 1024 x 1024
matrix_64x1024x1024 = torch.randint(0, 100, (64, 1024, 1024))

# - 128 x 512 x 512
matrix_128x512x512 = torch.randint(0, 100, (128, 512, 512))

# - 256 x 256 x 256
matrix_256x256x256 = torch.randint(0, 100, (256, 256, 256))

# –ó–∞–ø–æ–ª–Ω–∏—Ç–µ –∏—Ö —Å–ª—É—á–∞–π–Ω—ã–º–∏ —á–∏—Å–ª–∞–º–∏
```

### 3.2 –§—É–Ω–∫—Ü–∏—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ (5 –±–∞–ª–ª–æ–≤)
```python
# –°–æ–∑–¥–∞–π—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–π
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ torch.cuda.Event() –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –∏–∑–º–µ—Ä–µ–Ω–∏—è –Ω–∞ GPU
# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ time.time() –¥–ª—è –∏–∑–º–µ—Ä–µ–Ω–∏—è –Ω–∞ CPU
def measure_time(operation, *args, device='cpu', **kwargs):
    if isinstance(device, torch.device):
        device = str(device)
        
    if 'cuda' in device and torch.cuda.is_available():
        start = torch.cuda.Event(enable_timing=True)
        end = torch.cuda.Event(enable_timing=True)
        torch.cuda.synchronize()
        start.record()
        operation(*args, **kwargs)
        end.record()
        torch.cuda.synchronize()
        return start.elapsed_time(end) / 1000.0
    else:
        t0 = time.time()
        operation(*args, **kwargs)
        return time.time() - t0
```

### 3.3 –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–π (10 –±–∞–ª–ª–æ–≤)
```python
# –°—Ä–∞–≤–Ω–∏—Ç–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å–ª–µ–¥—É—é—â–∏—Ö –æ–ø–µ—Ä–∞—Ü–∏–π –Ω–∞ CPU –∏ CUDA:
# - –ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ (torch.matmul)
# - –ü–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–µ —Å–ª–æ–∂–µ–Ω–∏–µ
# - –ü–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ
# - –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
# - –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—É–º–º—ã –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
operations = [
    ("–ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ", lambda a, b: torch.matmul(a, b)),
    ("–ü–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–µ —Å–ª–æ–∂–µ–Ω–∏–µ", lambda a, b: a + b),
    ("–ü–æ—ç–ª–µ–º–µ–Ω—Ç–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ", lambda a, b: a * b),
    ("–¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ", lambda a, b: torch.transpose(a, -1, -2)),
    ("–°—É–º–º–∞ –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤", lambda a, b: torch.sum(a)),
]

# –î–ª—è –∫–∞–∂–¥–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏:
# 1. –ò–∑–º–µ—Ä—å—Ç–µ –≤—Ä–µ–º—è –Ω–∞ CPU
# 2. –ò–∑–º–µ—Ä—å—Ç–µ –≤—Ä–µ–º—è –Ω–∞ GPU (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
# 3. –í—ã—á–∏—Å–ª–∏—Ç–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ (speedup)
# 4. –í—ã–≤–µ–¥–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ç–∞–±–ª–∏—á–Ω–æ–º –≤–∏–¥–µ
print(f"{'–û–ø–µ—Ä–∞—Ü–∏—è':<22} | {'CPU (–º—Å)':<10} | {'GPU (–º—Å)':<10} | {'–£—Å–∫–æ—Ä–µ–Ω–∏–µ':<10}")
print("-" * 60)

A_cpu = matrix_64x1024x1024.clone()
B_cpu = matrix_64x1024x1024.clone()
cuda_available = torch.cuda.is_available()#–ü—Ä–∏–º–µ—Ä –¥–ª—è 1 –º–∞—Ç—Ä–∏—Ü—ã –∏–∑ 3
if cuda_available:
    A_gpu = A_cpu.cuda()
    B_gpu = B_cpu.cuda()
    
for name, op in operations:
    time_cpu_sec = measure_time(op, A_cpu, B_cpu, device='cpu')
    time_cpu_ms = time_cpu_sec * 1000

    if cuda_available:
        time_gpu_sec = measure_time(op, A_gpu, B_gpu, device='cuda')
        time_gpu_ms = time_gpu_sec * 1000
        speedup = time_cpu_ms / time_gpu_ms if time_gpu_ms > 0 else float('inf')
        speedup_str = f"{speedup:.1f}x"
    else:
        time_gpu_ms = float('nan')
        speedup_str = "N/A"

    if cuda_available:
        print(f"{name:<22} | {time_cpu_ms:<10.2f} | {time_gpu_ms:<10.2f} | {speedup_str:<10}")
    else:
        print(f"{name:<22} | {time_cpu_ms:<10.2f} | {'N/A':<10} | {speedup_str:<10}")
```

### –ü—Ä–∏–º–µ—Ä –≤—ã–≤–æ–¥–∞:
```
–û–ø–µ—Ä–∞—Ü–∏—è          | CPU (–º—Å) | GPU (–º—Å) | –£—Å–∫–æ—Ä–µ–Ω–∏–µ
–ú–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ|   150.2  |    12.3  |   12.2x
–°–ª–æ–∂–µ–Ω–∏–µ          |    45.1  |     3.2  |   14.1x
...
```

### 3.4 –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (5 –±–∞–ª–ª–æ–≤)
```python
# –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:
# - –ö–∞–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø–æ–ª—É—á–∞—é—Ç –Ω–∞–∏–±–æ–ª—å—à–µ–µ —É—Å–∫–æ—Ä–µ–Ω–∏–µ –Ω–∞ GPU?
GPU —É—Å–∫–æ—Ä—è–µ—Ç –ª—É—á—à–µ –≤—Å–µ–≥–æ —Ç–µ –æ–ø–µ—Ä–∞—Ü–∏–∏, –≥–¥–µ –º–Ω–æ–≥–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –¥–µ–ª–∞—Ç—å –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ.
–°–∏–ª—å–Ω–æ —É—Å–∫–æ—Ä—è—é—Ç—Å—è –Ω–∞ GPU –º–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ.

# - –ü–æ—á–µ–º—É –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ –Ω–∞ GPU?
–í—Ä–µ–º—è –Ω–∞ —á—Ç–µ–Ω–∏–µ –∏ –∑–∞–ø–∏—Å—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Ç—Ä–∞—Ç–∏—Ç—å—Å—è –±–æ–ª—å—à–µ, —á–µ–º –∏–¥–µ—Ç —Å–∞–º–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ. GPU –≤—ã–≥–æ–¥–µ–Ω –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π

# - –ö–∞–∫ —Ä–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü –≤–ª–∏—è–µ—Ç –Ω–∞ —É—Å–∫–æ—Ä–µ–Ω–∏–µ?
–†–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü —Å–∏–ª—å–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —É—Å–∫–æ—Ä–µ–Ω–∏–µ. –ù–∞ –º–∞–ª—ã—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö –Ω–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã (—Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è, –∑–∞–ø—É—Å–∫ —è–¥–µ—Ä) –¥–æ–º–∏–Ω–∏—Ä—É—é—Ç,
–ø–æ—ç—Ç–æ–º—É —É—Å–∫–æ—Ä–µ–Ω–∏—è –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å. –ù–∞ –∫—Ä—É–ø–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–∞—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ —Ä–∞—Å—Ç—ë—Ç –±—ã—Å—Ç—Ä–µ–µ, —á–µ–º –Ω–∞–∫–ª–∞–¥–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã,
–∏ GPU —Ä–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Å–≤–æ–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª.

# - –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏ –ø–µ—Ä–µ–¥–∞—á–µ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É CPU –∏ GPU?
–ü–µ—Ä–µ–¥–∞—á–∞ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É CPU –∏ GPU ‚Äî –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–∞—è –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è–º–∏ –Ω–∞ –æ–¥–Ω–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ.
–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–Ω–∑–æ—Ä–∞ —á–µ—Ä–µ–∑ .cuda() –∏–ª–∏ .cpu() –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ PCIe-—à–∏–Ω—É, –ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫–æ—Ç–æ—Ä–æ–π –Ω–∞ –ø–æ—Ä—è–¥–æ–∫ –Ω–∏–∂–µ,
—á–µ–º —É –ø–∞–º—è—Ç–∏ GPU. –ü–æ—ç—Ç–æ–º—É —á–∞—Å—Ç–∞—è –ø–µ—Ä–µ–¥–∞—á–∞ –¥–∞–Ω–Ω—ã—Ö —Å–≤–æ–¥–∏—Ç –Ω–∞ –Ω–µ—Ç –≤—Å—ë —É—Å–∫–æ—Ä–µ–Ω–∏–µ. 
```

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

1. **–ö–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —á–∏—Ç–∞–µ–º—ã–º** - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ –ø–æ–Ω—è—Ç–Ω—ã–µ –∏–º–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
2. **–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫** - –¥–æ–±–∞–≤—å—Ç–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–µ–π –∏ —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
3. **–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è** - –¥–æ–±–∞–≤—å—Ç–µ docstring –∫ —Ñ—É–Ω–∫—Ü–∏—è–º
4. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ** - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –ø—Ä–æ—Å—Ç—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö –∏–ª–∏ –Ω–∞–ø–∏—à–∏—Ç–µ —Ç–µ—Å—Ç—ã
5. **–ê–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç—å** - –∫–æ–¥ –¥–æ–ª–∂–µ–Ω —Ä–∞–±–æ—Ç–∞—Ç—å –∫–∞–∫ —Å GPU, —Ç–∞–∫ –∏ –±–µ–∑ –Ω–µ–≥–æ

## –°—Ä–æ–∫ —Å–¥–∞—á–∏
–î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –¥–æ –Ω–∞—á–∞–ª–∞ –∑–∞–Ω—è—Ç–∏—è 3.

## –ü–æ–ª–µ–∑–Ω—ã–µ —Å—Å—ã–ª–∫–∏
- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)
- [PyTorch Tutorials](https://pytorch.org/tutorials/)
- [Autograd Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)
- [CUDA Performance](https://pytorch.org/docs/stable/notes/cuda.html)

–£–¥–∞—á–∏ –≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–¥–∞–Ω–∏—è! üöÄ 